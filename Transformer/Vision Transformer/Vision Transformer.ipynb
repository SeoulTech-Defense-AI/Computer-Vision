{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cea81-092b-41c1-9a8c-b3494772002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 모듈\n",
    "import shutil  # 파일 및 디렉토리 작업을 위한 모듈\n",
    "import random  # 무작위 작업을 위한 모듈\n",
    "import scipy  # 과학 및 기술 계산을 위한 모듈\n",
    "import numpy as np  # 수치 연산을 위한 Python 라이브러리\n",
    "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리\n",
    "import tensorflow as tf  # 딥러닝을 위한 TensorFlow 라이브러리\n",
    "from tensorflow import keras  # TensorFlow의 고수준 API\n",
    "from keras import layers  # Keras의 레이어 클래스\n",
    "from keras.preprocessing.image import ImageDataGenerator  # 이미지 데이터 전처리 및 증강\n",
    "\n",
    "def generators(train_dir, val_dir, size=64, image_size=224):\n",
    "    \"\"\"학습 및 검증 데이터를 생성하는 함수\"\"\"\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255,  # 이미지의 픽셀 값을 0-1 범위로 정규화\n",
    "                                       rotation_range=180,  # 회전 범위 설정\n",
    "                                       width_shift_range=0.2,  # 가로 방향 이동 범위\n",
    "                                       height_shift_range=0.2,  # 세로 방향 이동 범위\n",
    "                                       shear_range=0.2,  # 시어 변환 범위\n",
    "                                       zoom_range=0.2,  # 줌 인/줌 아웃 범위\n",
    "                                       horizontal_flip=True,  # 가로 방향 뒤집기 활성화\n",
    "                                       vertical_flip=True,  # 세로 방향 뒤집기 활성화\n",
    "                                       fill_mode='nearest')  # 빈 공간 채우기 모드 설정\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1/255)  # 검증 데이터의 픽셀 값을 0-1 범위로 정규화\n",
    "\n",
    "    # 학습 데이터 생성기\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(image_size, image_size),  # 입력 이미지 크기 설정\n",
    "                                                        batch_size=size,  # 배치 크기 설정\n",
    "                                                        class_mode='categorical',  # 다중 클래스 분류 설정\n",
    "                                                        shuffle=True)  # 데이터 섞기 활성화\n",
    "\n",
    "    # 검증 데이터 생성기\n",
    "    val_generator = val_datagen.flow_from_directory(val_dir,\n",
    "                                                    target_size=(image_size, image_size),  # 입력 이미지 크기 설정\n",
    "                                                    batch_size=size,  # 배치 크기 설정\n",
    "                                                    class_mode='categorical')  # 다중 클래스 분류 설정\n",
    "\n",
    "    return train_generator, val_generator  # 학습 및 검증 데이터 생성기 반환\n",
    "\n",
    "# 데이터 디렉토리 설정 (경로는 your_path로 대체)\n",
    "train_dir = 'your_path'\n",
    "val_dir = 'your_path'\n",
    "\n",
    "# 데이터 생성기 호출\n",
    "train_generator, val_generator = generators(train_dir, val_dir)\n",
    "\n",
    "def create_patches(images, patch_size):\n",
    "    \"\"\"이미지를 패치로 분할하는 함수\"\"\"\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],  # 패치 크기 설정\n",
    "        strides=[1, patch_size, patch_size, 1],  # 패치 이동 거리 설정\n",
    "        rates=[1, 1, 1, 1],  # 패치 추출 속도 설정\n",
    "        padding='VALID')  # 패딩 설정\n",
    "    patches = tf.reshape(patches, [tf.shape(images)[0], -1, patch_size * patch_size * 3])  # 패치 재구성\n",
    "    return patches\n",
    "\n",
    "class patchEncoder(layers.Layer):\n",
    "    \"\"\"패치 인코더 레이어 클래스\"\"\"\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(patchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches  # 패치 수\n",
    "        self.projection = layers.Dense(units=projection_dim)  # 프로젝션 레이어\n",
    "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)  # 위치 임베딩\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)  # 패치 위치 계산\n",
    "        positions = tf.expand_dims(positions, axis=0)  # 차원 확장\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)  # 프로젝션 및 위치 임베딩 합산\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(patchEncoder, self).get_config()\n",
    "        config.update({'num_patches': self.num_patches, 'projection_dim': self.projection.units})  # 설정 업데이트\n",
    "        return config\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\"멀티 헤드 어텐션 레이어 클래스\"\"\"\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads  # 헤드 수\n",
    "        self.key_dim = key_dim  # 키 차원\n",
    "        self.projection_dim = key_dim // num_heads  # 프로젝션 차원\n",
    "\n",
    "        self.query_dense = layers.Dense(key_dim)  # 쿼리 밀집 레이어\n",
    "        self.key_dense = layers.Dense(key_dim)  # 키 밀집 레이어\n",
    "        self.value_dense = layers.Dense(key_dim)  # 값 밀집 레이어\n",
    "        self.combine_heads = layers.Dense(key_dim)  # 헤드 결합 레이어\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \"\"\"입력을 여러 헤드로 분할하는 함수\"\"\"\n",
    "        inputs = tf.reshape(inputs, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_length, projection_dim) 반환\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        query = self.query_dense(query)  # 쿼리 프로젝션\n",
    "        key = self.key_dense(key)  # 키 프로젝션\n",
    "        value = self.value_dense(value)  # 값 프로젝션\n",
    "\n",
    "        query = self.split_heads(query, batch_size)  # 쿼리 헤드 분할\n",
    "        key = self.split_heads(key, batch_size)  # 키 헤드 분할\n",
    "        value = self.split_heads(value, batch_size)  # 값 헤드 분할\n",
    "\n",
    "        score = tf.matmul(query, key, transpose_b=True)  # 점수 계산\n",
    "        score = score / tf.math.sqrt(tf.cast(self.projection_dim, tf.float32))  # 스케일링\n",
    "        weights = tf.nn.softmax(score, axis=-1)  # 소프트맥스 계산\n",
    "        attention = tf.matmul(weights, value)  # 어텐션 값 계산\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # 어텐션 결과 변환\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.key_dim))  # 헤드 결합\n",
    "\n",
    "        outputs = self.combine_heads(concat_attention)  # 최종 결과\n",
    "        return outputs\n",
    "\n",
    "def visionTransformer(input_shape,\n",
    "                      patch_size,\n",
    "                      num_patches,\n",
    "                      projection_dim,\n",
    "                      num_heads,\n",
    "                      num_transformer_layers,\n",
    "                      mlp_head_units,\n",
    "                      dropout_rate=.1,\n",
    "                      num_classes=10):\n",
    "    \"\"\"비전 트랜스포머 모델 정의 함수\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    patches = create_patches(inputs, patch_size)\n",
    "    encoded_patches = patchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    for _ in range(num_transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)  # 레이어 정규화\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x1, x1, x1)  # 어텐션 레이어\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])  # 스킵 연결\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)  # 레이어 정규화\n",
    "        x3 = layers.Dense(units=projection_dim, activation=tf.nn.gelu)(x3)  # MLP\n",
    "        x3 = layers.Dropout(dropout_rate)(x3)  # 드롭아웃\n",
    "        x3 = layers.Dense(units=projection_dim, activation=tf.nn.gelu)(x3)  # MLP\n",
    "        x3 = layers.Dropout(dropout_rate)(x3)  # 드롭아웃\n",
    "        encoded_patches = layers.Add()([x3, x2])  # 스킵 연결\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)  # 최종 레이어 정규화\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)  # 전역 평균 풀링\n",
    "\n",
    "    features = layers.Dense(units=mlp_head_units[0], activation=tf.nn.gelu)(representation)  # MLP 헤드\n",
    "    for units in mlp_head_units[1:]:\n",
    "        features = layers.Dense(units=units, activation=tf.nn.gelu)(features)\n",
    "    logits = layers.Dense(100)(features)  # ImageNet 100 클래스 예측\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)  # 모델 생성\n",
    "    return model\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_shape = (224, 224, 3)\n",
    "patch_size = 16\n",
    "num_patches = (input_shape[0] // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "num_transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "\n",
    "# 모델 생성\n",
    "vit = visionTransformer(input_shape=input_shape,\n",
    "                        patch_size=patch_size,\n",
    "                        num_patches=num_patches,\n",
    "                        projection_dim=projection_dim,\n",
    "                        num_heads=num_heads,\n",
    "                        num_transformer_layers=num_transformer_layers,\n",
    "                        mlp_head_units=mlp_head_units,\n",
    "                        dropout_rate=.3,\n",
    "                        num_classes=100)\n",
    "\n",
    "# 모델 컴파일\n",
    "vit.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-3, decay=3e-1),\n",
    "            loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# 체크포인트 및 조기 종료 설정 (경로는 your_path로 대체)\n",
    "model_path = 'your_path'\n",
    "check_point = keras.callbacks.ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = vit.fit(train_generator, batch_size=512, epochs=10000, validation_data=val_generator, callbacks=[check_point, early_stopping])\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Vision Transformer ImageNet 100')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'accuracy', 'val_loss', 'val_accuracy'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# 최고 검증 정확도 및 손실 출력\n",
    "print('Best Val Acc:', max(history.history['val_accuracy']))\n",
    "print('Best Val Loss:', min(history.history['val_loss']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
