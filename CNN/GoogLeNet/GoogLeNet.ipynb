{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 모듈\n",
    "import shutil  # 파일 및 디렉토리 작업을 위한 모듈\n",
    "import random  # 랜덤 작업을 위한 모듈\n",
    "import scipy  # 과학 및 기술 계산을 위한 모듈\n",
    "import numpy as np  # 수치 연산을 위한 Python 라이브러리\n",
    "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리\n",
    "from PIL import Image  # 이미지 처리 및 파일 입출력을 위한 라이브러리\n",
    "from keras.preprocessing.image import ImageDataGenerator  # 이미지 데이터 생성기 유틸리티\n",
    "from keras.models import Sequential, Model  # Keras의 시퀀셜 및 함수형 API 모델\n",
    "from keras.layers import (Dense, Dropout, Flatten, Conv2D, MaxPooling2D, \n",
    "                          BatchNormalization, Concatenate, AveragePooling2D, Input)  # 신경망의 레이어 구성 요소\n",
    "from keras.optimizers import SGD  # 최적화 알고리즘\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint  # 학습 중 콜백 함수\n",
    "\n",
    "def generators(train_dir, val_dir):\n",
    "    \"\"\"학습 및 검증 데이터를 생성하는 함수\"\"\"\n",
    "    # 학습 데이터 생성기 설정\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255,  # 이미지의 픽셀 값을 0-1 범위로 정규화\n",
    "                                       rotation_range=180,  # 최대 180도 회전\n",
    "                                       width_shift_range=0.2,  # 가로 방향 이동\n",
    "                                       height_shift_range=0.2,  # 세로 방향 이동\n",
    "                                       shear_range=0.2,  # 시어 변환\n",
    "                                       zoom_range=0.2,  # 줌 인/줌 아웃\n",
    "                                       horizontal_flip=True,  # 가로 방향 뒤집기\n",
    "                                       vertical_flip=True,  # 세로 방향 뒤집기\n",
    "                                       fill_mode='nearest')  # 변환 중 생기는 빈 공간을 주변의 유사한 픽셀 값으로 채우기\n",
    "\n",
    "    # 검증 데이터 생성기 설정\n",
    "    val_datagen = ImageDataGenerator(rescale=1/255)  # 이미지의 픽셀 값을 0-1 범위로 정규화\n",
    "\n",
    "    # 학습 데이터 생성\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir,  # 학습 데이터가 위치한 디렉토리\n",
    "                                                        target_size=(227, 227),  # 입력 이미지 크기\n",
    "                                                        batch_size=512,  # 배치 크기\n",
    "                                                        class_mode='categorical',  # 다중 클래스 분류\n",
    "                                                        shuffle=True)  # 데이터를 섞어서 배치 생성\n",
    "\n",
    "    # 검증 데이터 생성\n",
    "    val_generator = val_datagen.flow_from_directory(val_dir,  # 검증 데이터가 위치한 디렉토리\n",
    "                                                    target_size=(227, 227),  # 입력 이미지 크기\n",
    "                                                    batch_size=512,  # 배치 크기\n",
    "                                                    class_mode='categorical')  # 다중 클래스 분류\n",
    "\n",
    "    return train_generator, val_generator  # 학습 및 검증 데이터 생성기 반환\n",
    "\n",
    "# 학습 및 검증 데이터 디렉토리 설정\n",
    "train_dir = 'your_path'  # 학습 데이터 디렉토리 경로\n",
    "val_dir = 'your_path'  # 검증 데이터 디렉토리 경로\n",
    "\n",
    "# 데이터 생성기 생성\n",
    "train_generator, val_generator = generators(train_dir, val_dir)\n",
    "\n",
    "def inception(x, filters):\n",
    "    \"\"\"GoogLeNet의 Inception 모듈을 정의하는 함수\"\"\"\n",
    "    # 1x1 Conv\n",
    "    path1 = Conv2D(filters=filters[0], kernel_size=(1, 1), strides=1, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # 1x1 Conv -> 3x3 Conv\n",
    "    path2 = Conv2D(filters=filters[1][0], kernel_size=(1, 1), strides=1, padding='same', activation='relu')(x)\n",
    "    path2 = Conv2D(filters=filters[1][1], kernel_size=(3, 3), strides=1, padding='same', activation='relu')(path2)\n",
    "    \n",
    "    # 1x1 Conv -> 5x5 Conv\n",
    "    path3 = Conv2D(filters=filters[2][0], kernel_size=(1, 1), strides=1, padding='same', activation='relu')(x)\n",
    "    path3 = Conv2D(filters=filters[2][1], kernel_size=(5, 5), strides=1, padding='same', activation='relu')(path3)\n",
    "    \n",
    "    # 3x3 MaxPooling -> 1x1 Conv\n",
    "    path4 = MaxPooling2D(pool_size=(3, 3), strides=1, padding='same')(x)\n",
    "    path4 = Conv2D(filters=filters[3], kernel_size=(1, 1), strides=1, padding='same', activation='relu')(path4)\n",
    "    \n",
    "    # 각 경로의 결과를 Concatenate로 병합\n",
    "    return Concatenate(axis=-1)([path1, path2, path3, path4])\n",
    "\n",
    "def auxiliary(x, name=None):\n",
    "    \"\"\"GoogLeNet의 Auxiliary Classifier를 정의하는 함수\"\"\"\n",
    "    layer = AveragePooling2D((5, 5), strides=3, padding='valid')(x)  # 5x5 AveragePooling\n",
    "    layer = Conv2D(128, (1, 1), 1, padding='same', activation='relu')(layer)  # 1x1 Conv\n",
    "    layer = Flatten()(layer)  # 평탄화\n",
    "    layer = Dense(256, activation='relu')(layer)  # FC Layer 1\n",
    "    layer = Dropout(0.4)(layer)  # 드롭아웃\n",
    "    layer = Dense(1000, activation='softmax', name=name)(layer)  # FC Layer 2 및 softmax 활성화 함수\n",
    "    return layer\n",
    "\n",
    "def GoogLeNet():\n",
    "    \"\"\"GoogLeNet 모델 정의\"\"\"\n",
    "    layer_in = Input(shape=(224, 224, 3))  # 입력 레이어\n",
    "    layer = Conv2D(filters=64, kernel_size=(7, 7), strides=2, padding='same', activation='relu')(layer_in)  # 7x7 Conv\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(layer)  # 3x3 MaxPooling\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "\n",
    "    layer = Conv2D(filters=64, kernel_size=(1, 1), strides=1, padding='same', activation='relu')(layer)  # 1x1 Conv\n",
    "    layer = Conv2D(filters=192, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(layer)  # 3x3 Conv\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(layer)  # 3x3 MaxPooling\n",
    "\n",
    "    layer = inception(layer, [64, (96, 128), (16, 32), 32])  # Inception 모듈\n",
    "    layer = inception(layer, [128, (128, 192), (32, 96), 64])  # Inception 모듈\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(layer)  # 3x3 MaxPooling\n",
    "\n",
    "    layer = inception(layer, [192, (96, 208), (16, 48), 64])  # Inception 모듈\n",
    "    aux1 = auxiliary(layer, name='aux1')  # 첫 번째 Auxiliary Classifier\n",
    "    layer = inception(layer, [160, (112, 224), (24, 64), 64])  # Inception 모듈\n",
    "    layer = inception(layer, [128, (128, 256), (24, 64), 64])  # Inception 모듈\n",
    "    layer = inception(layer, [112, (144, 288), (32, 64), 64])  # Inception 모듈\n",
    "    aux2 = auxiliary(layer, name='aux2')  # 두 번째 Auxiliary Classifier\n",
    "    layer = inception(layer, [256, (160, 320), (32, 128), 128])  # Inception 모듈\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(layer)  # 3x3 MaxPooling\n",
    "\n",
    "    layer = inception(layer, [256, (160, 320), (32, 128), 128])  # Inception 모듈\n",
    "    layer = inception(layer, [384, (192, 384), (48, 128), 128])  # Inception 모듈\n",
    "    layer = AveragePooling2D(pool_size=(7, 7), strides=1, padding='valid')(layer)  # 7x7 AveragePooling\n",
    "\n",
    "    layer = Flatten()(layer)  # 평탄화\n",
    "    layer = Dropout(0.4)(layer)  # 드롭아웃\n",
    "    layer = Dense(units=256, activation='linear')(layer)  # FC Layer 1\n",
    "    main = Dense(units=1000, activation='softmax', name='main')(layer)  # FC Layer 2 및 softmax 활성화 함수\n",
    "\n",
    "    model = Model(inputs=layer_in, outputs=[main, aux1, aux2])  # GoogLeNet 모델 정의\n",
    "    model.compile(optimizer=SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])  # 모델 컴파일\n",
    "    model.summary()  # 모델 구조 출력\n",
    "\n",
    "    return model\n",
    "\n",
    "# GoogLeNet 모델 생성\n",
    "model = GoogLeNet()\n",
    "\n",
    "# 모델 저장 경로 및 콜백 설정\n",
    "model_path = 'your_path'  # 최적의 모델을 저장할 파일 경로\n",
    "CP = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)  # 최적의 모델을 저장하는 콜백\n",
    "ES = EarlyStopping(monitor='val_loss', patience=10)  # 조기 종료를 위한 콜백, 10 에포크 동안 개선이 없으면 학습 중단\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(train_generator,  # 학습 데이터 생성기\n",
    "                    epochs=100,  # 최대 100 에포크 동안 학습\n",
    "                    validation_data=val_generator,  # 검증 데이터 생성기\n",
    "                    callbacks=[CP, ES])  # 콜백 함수들\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.plot(history.history['loss'])  # 학습 손실 그래프\n",
    "plt.plot(history.history['accuracy'])  # 학습 정확도 그래프\n",
    "plt.plot(history.history['val_loss'])  # 검증 손실 그래프\n",
    "plt.plot(history.history['val_accuracy'])  # 검증 정확도 그래프\n",
    "plt.title('GoogLeNet')  # 그래프 제목\n",
    "plt.ylabel('loss')  # y축 라벨\n",
    "plt.xlabel('epoch')  # x축 라벨\n",
    "plt.legend(['loss', 'val_loss', 'accuracy', 'val_accuracy'], loc='upper left')  # 범례\n",
    "plt.show()  # 그래프 출력\n",
    "\n",
    "# 최적의 검증 정확도와 손실 출력\n",
    "print('Best Val Acc:', max(history.history['val_accuracy']))  # 최고 검증 정확도 출력\n",
    "print('Best Val Loss:', min(history.history['val_loss']))  # 최저 검증 손실 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
