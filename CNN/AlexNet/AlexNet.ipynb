{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 모듈\n",
    "import shutil  # 파일 및 디렉토리 작업을 위한 모듈\n",
    "import random  # 랜덤 작업을 위한 모듈\n",
    "import scipy  # 과학 및 기술 계산을 위한 모듈\n",
    "import numpy as np  # 수치 연산을 위한 Python 라이브러리\n",
    "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리\n",
    "from PIL import Image  # 이미지 처리 및 파일 입출력을 위한 라이브러리\n",
    "from keras.preprocessing.image import ImageDataGenerator  # 이미지 데이터 생성기 유틸리티\n",
    "from keras.models import Sequential, Model  # Keras의 시퀀셜 및 함수형 API 모델\n",
    "from keras.layers import (Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D, \n",
    "                          BatchNormalization)  # 신경망의 레이어 구성 요소\n",
    "from keras.optimizers import Adam, SGD  # 최적화 알고리즘\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint  # 학습 중 콜백 함수\n",
    "\n",
    "def generators(train_dir, val_dir):\n",
    "    \"\"\"학습 및 검증 데이터를 생성하는 함수\"\"\"\n",
    "    # 학습 데이터 생성기 설정\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255,  # 이미지의 픽셀 값을 0-1 범위로 정규화\n",
    "                                       rotation_range=180,  # 최대 180도 회전\n",
    "                                       width_shift_range=0.2,  # 가로 방향 이동\n",
    "                                       height_shift_range=0.2,  # 세로 방향 이동\n",
    "                                       shear_range=0.2,  # 시어 변환\n",
    "                                       zoom_range=0.2,  # 줌 인/줌 아웃\n",
    "                                       horizontal_flip=True,  # 가로 방향 뒤집기\n",
    "                                       vertical_flip=True,  # 세로 방향 뒤집기\n",
    "                                       fill_mode='nearest')  # 변환 중 생기는 빈 공간을 주변의 유사한 픽셀 값으로 채우기\n",
    "\n",
    "    # 검증 데이터 생성기 설정\n",
    "    val_datagen = ImageDataGenerator(rescale=1/255)  # 이미지의 픽셀 값을 0-1 범위로 정규화\n",
    "\n",
    "    # 학습 데이터 생성\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir,  # 학습 데이터가 위치한 디렉토리\n",
    "                                                        target_size=(227, 227),  # 입력 이미지 크기\n",
    "                                                        batch_size=512,  # 배치 크기\n",
    "                                                        class_mode='categorical',  # 다중 클래스 분류\n",
    "                                                        shuffle=True)  # 데이터를 섞어서 배치 생성\n",
    "\n",
    "    # 검증 데이터 생성\n",
    "    val_generator = val_datagen.flow_from_directory(val_dir,  # 검증 데이터가 위치한 디렉토리\n",
    "                                                    target_size=(227, 227),  # 입력 이미지 크기\n",
    "                                                    batch_size=512,  # 배치 크기\n",
    "                                                    class_mode='categorical')  # 다중 클래스 분류\n",
    "\n",
    "    return train_generator, val_generator  # 학습 및 검증 데이터 생성기 반환\n",
    "\n",
    "# 학습 및 검증 데이터 디렉토리 설정\n",
    "train_dir = 'your_path'\n",
    "val_dir = 'your_path'\n",
    "\n",
    "# 데이터 생성기 생성\n",
    "train_generator, val_generator = generators(train_dir, val_dir)\n",
    "\n",
    "def alexnet():\n",
    "    \"\"\"AlexNet 모델을 정의하는 함수\"\"\"\n",
    "    input_tensor = Input(shape=(227, 227, 3))  # 입력 텐서 정의, 입력 크기는 227x227 RGB 이미지\n",
    "\n",
    "    # 첫 번째 Convolutional Layer\n",
    "    layer = Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu')(input_tensor)\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(layer)  # 최대 풀링\n",
    "\n",
    "    # 두 번째 Convolutional Layer\n",
    "    layer = Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(layer)  # 최대 풀링\n",
    "\n",
    "    # 세 번째, 네 번째, 다섯 번째 Convolutional Layer\n",
    "    layer = Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "    \n",
    "    layer = Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "\n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)  # 배치 정규화\n",
    "    layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(layer)  # 최대 풀링\n",
    "    \n",
    "    layer = Flatten()(layer)  # 1차원 벡터로 평탄화\n",
    "\n",
    "    # 첫 번째 Fully Connected Layer\n",
    "    layer = Dense(units=4096, activation='relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)  # 드롭아웃을 적용하여 과적합 방지\n",
    "\n",
    "    # 두 번째 Fully Connected Layer\n",
    "    layer = Dense(units=4096, activation='relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)  # 드롭아웃을 적용하여 과적합 방지\n",
    "\n",
    "    # 출력 Layer\n",
    "    output = Dense(units=100, activation='softmax')(layer)  # 100개의 클래스에 대한 확률 분포를 출력\n",
    "\n",
    "    # 모델 생성 및 컴파일\n",
    "    model = Model(inputs=input_tensor, outputs=output)  # 입력 텐서와 출력 텐서를 사용하여 모델 정의\n",
    "    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9, decay=0.0005),  # SGD 옵티마이저 사용\n",
    "                  loss='categorical_crossentropy',  # 다중 클래스 분류를 위한 손실 함수\n",
    "                  metrics=['accuracy'])  # 정확도를 평가 메트릭으로 사용\n",
    "    model.summary()  # 모델 구조 요약 출력\n",
    "    \n",
    "    return model  # 생성된 모델 반환\n",
    "\n",
    "# AlexNet 모델 생성\n",
    "model = alexnet()\n",
    "\n",
    "# 모델 저장 경로 및 콜백 설정\n",
    "model_path = 'your_path'  # 최적의 모델을 저장할 파일 경로\n",
    "CP = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)  # 최적의 모델을 저장하는 콜백\n",
    "ES = EarlyStopping(monitor='val_loss', patience=10)  # 조기 종료를 위한 콜백, 10 에포크 동안 개선이 없으면 학습 중단\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(train_generator,  # 학습 데이터 생성기\n",
    "                    epochs=100,  # 최대 100 에포크 동안 학습\n",
    "                    validation_data=val_generator,  # 검증 데이터 생성기\n",
    "                    callbacks=[CP, ES])  # 콜백 함수들\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.plot(history.history['loss'])  # 학습 손실 그래프\n",
    "plt.plot(history.history['accuracy'])  # 학습 정확도 그래프\n",
    "plt.plot(history.history['val_loss'])  # 검증 손실 그래프\n",
    "plt.plot(history.history['val_accuracy'])  # 검증 정확도 그래프\n",
    "plt.title('AlexNet')  # 그래프 제목\n",
    "plt.ylabel('loss')  # y축 라벨\n",
    "plt.xlabel('epoch')  # x축 라벨\n",
    "plt.legend(['loss', 'val_loss', 'accuracy', 'val_accuracy'], loc='upper left')  # 범례\n",
    "plt.show()  # 그래프 출력\n",
    "\n",
    "# 최적의 검증 정확도와 손실 출력\n",
    "print('Best Val Acc:', max(history.history['val_accuracy']))  # 최고 검증 정확도 출력\n",
    "print('Best Val Loss:', min(history.history['val_loss']))  # 최저 검증 손실 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
